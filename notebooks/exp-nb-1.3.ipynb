{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd2dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "SEED = 42\n",
    "n_splits = 8\n",
    "n_estimators=5000\n",
    "early_stopping_rounds = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0352edb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape : (52500, 11)\n"
     ]
    }
   ],
   "source": [
    "# train_data = pd.read_csv('../data/raw/train.csv')  #index_col='id'\n",
    "# test_data = pd.read_csv('../data/raw/test.csv') # , index_col='id'\n",
    "data = pd.read_csv('../data/raw/podcast_dataset.csv')\n",
    "\n",
    "# print(\"train_data shape :\",train_data.shape)\n",
    "# print(\"test_data shape :\",test_data.shape)\n",
    "print(\"data shape :\",data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69966f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape after dropping na and duplicates : (52500, 11)\n"
     ]
    }
   ],
   "source": [
    "TARGET = 'Listening_Time_minutes'\n",
    "data_clean = data.dropna(subset=[TARGET]).drop_duplicates()\n",
    "print(\"data shape after dropping na and duplicates :\",data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b83379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (35894, 10)\n",
      "Test  shape: (8974, 10)\n",
      "Orig_clean  shape: (44868, 11)\n"
     ]
    }
   ],
   "source": [
    "X= data_clean.drop(columns=[TARGET])\n",
    "y= data_clean[TARGET]\n",
    "train, test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test  shape: {test.shape}\")\n",
    "print(f\"Orig_clean  shape: {data_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a871f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class PodcastPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.genre_mapping = {\n",
    "            'Music': 0, 'True Crime': 1, 'Health': 2, 'Education': 3,\n",
    "            'Technology': 4, 'Business': 5, 'Lifestyle': 6,\n",
    "            'Sports': 7, 'Comedy': 8, 'News': 9\n",
    "        }\n",
    "        self.day_mapping = {\n",
    "            'Tuesday': 0, 'Monday': 1, 'Wednesday': 2,\n",
    "            'Saturday': 3, 'Friday': 4, 'Thursday': 5, 'Sunday': 6\n",
    "        }\n",
    "        self.time_mapping = {\n",
    "            'Night': 0, 'Afternoon': 1, 'Morning': 2, 'Evening': 3\n",
    "        }\n",
    "        self.label_encoders = {}\n",
    "        self.num_medians = {}\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def _data_process(self, df):\n",
    "        df = df.copy()\n",
    "\n",
    "        # Your feature engineering\n",
    "        df['Episode_Title_num'] = (\n",
    "            df['Episode_Title'].astype(str).str.replace('Episode ', '', regex=False).astype(int)\n",
    "        )\n",
    "        # numeric medians applied later\n",
    "        df['Episode_Sentiment'] = df['Episode_Sentiment'].replace(\n",
    "            {'Neutral': 0, 'Positive': 1, 'Negative': -1}\n",
    "        )\n",
    "\n",
    "        df['Ad_Density'] = df['Number_of_Ads'] / (df['Episode_Length_minutes'] + 1e-3)\n",
    "        df['Popularity_Diff'] = df['Host_Popularity_percentage'] - df['Guest_Popularity_percentage']\n",
    "        df['Popularity_Interaction'] = df['Host_Popularity_percentage'] * df['Guest_Popularity_percentage']\n",
    "        df['Host_Popularity_squared'] = df['Host_Popularity_percentage'] ** 2\n",
    "        df['Popularity_Average'] = (\n",
    "            df['Host_Popularity_percentage'] + df['Guest_Popularity_percentage']\n",
    "        ) / 2\n",
    "        \n",
    "        df['Genre_Num'] = df['Genre'].map(self.genre_mapping)\n",
    "        df['Publication_Day_Num'] = df['Publication_Day'].map(self.day_mapping)\n",
    "        df['Publication_Time_Num'] = df['Publication_Time'].map(self.time_mapping)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self._data_process(X)\n",
    "\n",
    "        # 1) Fit TF-IDF on Podcast_Name\n",
    "        tfidf_train = self.vectorizer.fit_transform(X['Podcast_Name'])\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            tfidf_train.toarray(),\n",
    "            columns=self.vectorizer.get_feature_names_out(),\n",
    "            index=X.index\n",
    "        )\n",
    "\n",
    "        # 2) Fill numeric medians and store them\n",
    "        num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "        for col in num_cols:\n",
    "            median_val = X[col].median()\n",
    "            self.num_medians[col] = median_val\n",
    "            X[col] = X[col].fillna(median_val)\n",
    "\n",
    "        # 3) Label encode categorical columns\n",
    "        cat_cols = X.select_dtypes(exclude=['number']).columns.tolist()\n",
    "        X[cat_cols] = X[cat_cols].fillna(\"Missing\")\n",
    "        for col in cat_cols:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col])\n",
    "            self.label_encoders[col] = le\n",
    "\n",
    "        # 4) Combine tabular + TF-IDF and remember feature order\n",
    "        X_full = pd.concat([X, tfidf_df], axis=1)\n",
    "        self.feature_names_ = X_full.columns.tolist()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self._data_process(X)\n",
    "\n",
    "        # 1) TF-IDF using existing vocab\n",
    "        tfidf_test = self.vectorizer.transform(X['Podcast_Name'])\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            tfidf_test.toarray(),\n",
    "            columns=self.vectorizer.get_feature_names_out(),\n",
    "            index=X.index\n",
    "        )\n",
    "\n",
    "        # 2) Fill numeric using training medians\n",
    "        for col, median_val in self.num_medians.items():\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].fillna(median_val)\n",
    "\n",
    "        # 3) Apply label encoders (handle unknowns as \"Missing\" if needed)\n",
    "        cat_cols = X.select_dtypes(exclude=['number']).columns.tolist()\n",
    "        X[cat_cols] = X[cat_cols].fillna(\"Missing\")\n",
    "        for col, le in self.label_encoders.items():\n",
    "            # Map unknown labels to a fallback if required\n",
    "            X[col] = X[col].map(lambda v: v if v in le.classes_ else \"Missing\")\n",
    "            # Ensure encoder knows \"Missing\"\n",
    "            if \"Missing\" not in le.classes_:\n",
    "                le.classes_ = np.append(le.classes_, \"Missing\")\n",
    "            X[col] = le.transform(X[col])\n",
    "\n",
    "        X_full = pd.concat([X, tfidf_df], axis=1)\n",
    "\n",
    "        # Reindex to match training feature order\n",
    "        X_full = X_full.reindex(columns=self.feature_names_, fill_value=0)\n",
    "\n",
    "        return X_full.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf900ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "preprocessor = PodcastPreprocessor()\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b448f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['podcast_model.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe.fit(train, y_train)\n",
    "preprocessor = PodcastPreprocessor()\n",
    "X_train_transformed = preprocessor.fit_transform(train)\n",
    "joblib.dump(preprocessor, \"podcast_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a53a20d",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3b8d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "xgb_base = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    random_state=SEED,\n",
    "    tree_method=\"hist\",   # or gpu_hist\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c5ead12",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    \"n_estimators\": [300, 500, 800, 1000],\n",
    "    \"max_depth\": [6, 8, 10, 12, 15],\n",
    "    \"learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.5, 0.7, 0.9],\n",
    "    \"reg_alpha\": [0.0, 0.5, 0.8, 1.0],\n",
    "    \"reg_lambda\": [1.0, 2.0, 4.0, 6.0],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40507e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as NitinNandeshwar\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as NitinNandeshwar\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"NitinNandeshwar/Podcast-Listening-Time-Prediction\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"NitinNandeshwar/Podcast-Listening-Time-Prediction\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository NitinNandeshwar/Podcast-Listening-Time-Prediction initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository NitinNandeshwar/Podcast-Listening-Time-Prediction initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/68a4d8b968d247d7bec901a8bbcaccfa', creation_time=1765668177842, experiment_id='0', last_update_time=1765668177842, lifecycle_stage='active', name='xgboost_podcast_regression', tags={}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dagshub\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri('https://dagshub.com/NitinNandeshwar/Podcast-Listening-Time-Prediction.mlflow')\n",
    "dagshub.init(repo_owner='NitinNandeshwar', repo_name='Podcast-Listening-Time-Prediction', mlflow=True)\n",
    "\n",
    "mlflow.set_experiment(\"xgboost_podcast_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c523b852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/13 23:25:22 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of xgboost. If you encounter errors during autologging, try upgrading / downgrading xgboost to a supported version, or try upgrading MLflow.\n",
      "2025/12/13 23:25:22 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n",
      "2025/12/13 23:25:22 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of lightgbm. If you encounter errors during autologging, try upgrading / downgrading lightgbm to a supported version, or try upgrading MLflow.\n",
      "2025/12/13 23:25:22 INFO mlflow.tracking.fluent: Autologging successfully enabled for lightgbm.\n",
      "2025/12/13 23:25:22 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2025/12/13 23:25:22 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/13 23:53:10 INFO mlflow.sklearn.utils: Logging the 5 best runs, 25 runs will be omitted.\n",
      "2025/12/13 23:53:14 INFO mlflow.tracking._tracking_service.client: üèÉ View run serious-deer-952 at: https://dagshub.com/NitinNandeshwar/Podcast-Listening-Time-Prediction.mlflow/#/experiments/0/runs/15d9f3e216c24a27bd3a7b668191d1a9.\n",
      "2025/12/13 23:53:14 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://dagshub.com/NitinNandeshwar/Podcast-Listening-Time-Prediction.mlflow/#/experiments/0.\n",
      "2025/12/13 23:53:16 INFO mlflow.tracking._tracking_service.client: üèÉ View run languid-shrimp-113 at: https://dagshub.com/NitinNandeshwar/Podcast-Listening-Time-Prediction.mlflow/#/experiments/0/runs/91fb55c81fc54f79a5c0bf03f8d24ee7.\n",
      "2025/12/13 23:53:16 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://dagshub.com/NitinNandeshwar/Podcast-Listening-Time-Prediction.mlflow/#/experiments/0.\n",
      "2025/12/13 23:53:17 INFO mlflow.tracking._tracking_service.client: üèÉ View run unleashed-sheep-740 at: https://dagshub.com/NitinNandeshwar/Podcast-Listening-Time-Prediction.mlflow/#/experiments/0/runs/b8af5f4e259547d687c253a6d1bfe722.\n",
      "2025/12/13 23:53:17 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://dagshub.com/NitinNandeshwar/Podcast-Listening-Time-Prediction.mlflow/#/experiments/0.\n",
      "2025/12/13 23:53:18 INFO mlflow.tracking._tracking_service.client: üèÉ View run silent-stork-809 at: https://dagshub.com/NitinNandeshwar/Podcast-Listening-Time-Prediction.mlflow/#/experiments/0/runs/8d92d8a033f9494bac244892fae11037.\n",
      "2025/12/13 23:53:18 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://dagshub.com/NitinNandeshwar/Podcast-Listening-Time-Prediction.mlflow/#/experiments/0.\n",
      "2025/12/13 23:53:20 INFO mlflow.tracking._tracking_service.client: üèÉ View run efficient-roo-292 at: https://dagshub.com/NitinNandeshwar/Podcast-Listening-Time-Prediction.mlflow/#/experiments/0/runs/c43e774d10ac41a084ff99cf11c4f61f.\n",
      "2025/12/13 23:53:20 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://dagshub.com/NitinNandeshwar/Podcast-Listening-Time-Prediction.mlflow/#/experiments/0.\n",
      "2025/12/13 23:53:27 INFO mlflow.tracking._tracking_service.client: üèÉ View run xgb_random_search at: https://dagshub.com/NitinNandeshwar/Podcast-Listening-Time-Prediction.mlflow/#/experiments/0/runs/aac477ca7e8844fbaed600646db1100b.\n",
      "2025/12/13 23:53:27 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://dagshub.com/NitinNandeshwar/Podcast-Listening-Time-Prediction.mlflow/#/experiments/0.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=kf,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"xgb_random_search\"):\n",
    "    mlflow.autolog()\n",
    "\n",
    "    search.fit(X_train_transformed, y_train)\n",
    "\n",
    "    best_rmse = -search.best_score_\n",
    "    best_params = search.best_params_\n",
    "\n",
    "    mlflow.log_metric(\"best_cv_rmse\", best_rmse)\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    best_model = search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6af682fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 1.0,\n",
       " 'reg_lambda': 1.0,\n",
       " 'reg_alpha': 0.8,\n",
       " 'n_estimators': 300,\n",
       " 'max_depth': 6,\n",
       " 'learning_rate': 0.05,\n",
       " 'colsample_bytree': 0.9}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd9e9142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.665979088451019"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fceff1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_model, \"model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caec3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_artifact(\"podcast_model.pkl\")\n",
    "mlflow.log_artifact(\"model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309c4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "podcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
