{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd2dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "SEED = 42\n",
    "n_splits = 8\n",
    "n_estimators=5000\n",
    "early_stopping_rounds = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0352edb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape : (52500, 11)\n"
     ]
    }
   ],
   "source": [
    "# train_data = pd.read_csv('../data/raw/train.csv')  #index_col='id'\n",
    "# test_data = pd.read_csv('../data/raw/test.csv') # , index_col='id'\n",
    "data = pd.read_csv('../data/raw/podcast_dataset.csv')\n",
    "\n",
    "# print(\"train_data shape :\",train_data.shape)\n",
    "# print(\"test_data shape :\",test_data.shape)\n",
    "print(\"data shape :\",data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69966f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape after dropping na and duplicates : (52500, 11)\n"
     ]
    }
   ],
   "source": [
    "TARGET = 'Listening_Time_minutes'\n",
    "data_clean = data.dropna(subset=[TARGET]).drop_duplicates()\n",
    "print(\"data shape after dropping na and duplicates :\",data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b83379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (35894, 10)\n",
      "Test  shape: (8974, 10)\n",
      "Orig_clean  shape: (44868, 11)\n"
     ]
    }
   ],
   "source": [
    "X= data_clean.drop(columns=[TARGET])\n",
    "y= data_clean[TARGET]\n",
    "train, test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test  shape: {test.shape}\")\n",
    "print(f\"Orig_clean  shape: {data_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a871f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class PodcastPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.genre_mapping = {\n",
    "            'Music': 0, 'True Crime': 1, 'Health': 2, 'Education': 3,\n",
    "            'Technology': 4, 'Business': 5, 'Lifestyle': 6,\n",
    "            'Sports': 7, 'Comedy': 8, 'News': 9\n",
    "        }\n",
    "        self.day_mapping = {\n",
    "            'Tuesday': 0, 'Monday': 1, 'Wednesday': 2,\n",
    "            'Saturday': 3, 'Friday': 4, 'Thursday': 5, 'Sunday': 6\n",
    "        }\n",
    "        self.time_mapping = {\n",
    "            'Night': 0, 'Afternoon': 1, 'Morning': 2, 'Evening': 3\n",
    "        }\n",
    "        self.label_encoders = {}\n",
    "        self.num_medians = {}\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def _data_process(self, df):\n",
    "        df = df.copy()\n",
    "\n",
    "        # Your feature engineering\n",
    "        df['Episode_Title_num'] = (\n",
    "            df['Episode_Title'].astype(str).str.replace('Episode ', '', regex=False).astype(int)\n",
    "        )\n",
    "        # numeric medians applied later\n",
    "        df['Episode_Sentiment'] = df['Episode_Sentiment'].replace(\n",
    "            {'Neutral': 0, 'Positive': 1, 'Negative': -1}\n",
    "        )\n",
    "\n",
    "        df['Ad_Density'] = df['Number_of_Ads'] / (df['Episode_Length_minutes'] + 1e-3)\n",
    "        df['Popularity_Diff'] = df['Host_Popularity_percentage'] - df['Guest_Popularity_percentage']\n",
    "        df['Popularity_Interaction'] = df['Host_Popularity_percentage'] * df['Guest_Popularity_percentage']\n",
    "        df['Host_Popularity_squared'] = df['Host_Popularity_percentage'] ** 2\n",
    "        df['Popularity_Average'] = (\n",
    "            df['Host_Popularity_percentage'] + df['Guest_Popularity_percentage']\n",
    "        ) / 2\n",
    "        \n",
    "        df['Genre_Num'] = df['Genre'].map(self.genre_mapping)\n",
    "        df['Publication_Day_Num'] = df['Publication_Day'].map(self.day_mapping)\n",
    "        df['Publication_Time_Num'] = df['Publication_Time'].map(self.time_mapping)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = self._data_process(X)\n",
    "\n",
    "        # 1) Fit TF-IDF on Podcast_Name\n",
    "        tfidf_train = self.vectorizer.fit_transform(X['Podcast_Name'])\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            tfidf_train.toarray(),\n",
    "            columns=self.vectorizer.get_feature_names_out(),\n",
    "            index=X.index\n",
    "        )\n",
    "\n",
    "        # 2) Fill numeric medians and store them\n",
    "        num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "        for col in num_cols:\n",
    "            median_val = X[col].median()\n",
    "            self.num_medians[col] = median_val\n",
    "            X[col] = X[col].fillna(median_val)\n",
    "\n",
    "        # 3) Label encode categorical columns\n",
    "        cat_cols = X.select_dtypes(exclude=['number']).columns.tolist()\n",
    "        X[cat_cols] = X[cat_cols].fillna(\"Missing\")\n",
    "        for col in cat_cols:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col])\n",
    "            self.label_encoders[col] = le\n",
    "\n",
    "        # 4) Combine tabular + TF-IDF and remember feature order\n",
    "        X_full = pd.concat([X, tfidf_df], axis=1)\n",
    "        self.feature_names_ = X_full.columns.tolist()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self._data_process(X)\n",
    "\n",
    "        # 1) TF-IDF using existing vocab\n",
    "        tfidf_test = self.vectorizer.transform(X['Podcast_Name'])\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            tfidf_test.toarray(),\n",
    "            columns=self.vectorizer.get_feature_names_out(),\n",
    "            index=X.index\n",
    "        )\n",
    "\n",
    "        # 2) Fill numeric using training medians\n",
    "        for col, median_val in self.num_medians.items():\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].fillna(median_val)\n",
    "\n",
    "        # 3) Apply label encoders (handle unknowns as \"Missing\" if needed)\n",
    "        cat_cols = X.select_dtypes(exclude=['number']).columns.tolist()\n",
    "        X[cat_cols] = X[cat_cols].fillna(\"Missing\")\n",
    "        for col, le in self.label_encoders.items():\n",
    "            # Map unknown labels to a fallback if required\n",
    "            X[col] = X[col].map(lambda v: v if v in le.classes_ else \"Missing\")\n",
    "            # Ensure encoder knows \"Missing\"\n",
    "            if \"Missing\" not in le.classes_:\n",
    "                le.classes_ = np.append(le.classes_, \"Missing\")\n",
    "            X[col] = le.transform(X[col])\n",
    "\n",
    "        X_full = pd.concat([X, tfidf_df], axis=1)\n",
    "\n",
    "        # Reindex to match training feature order\n",
    "        X_full = X_full.reindex(columns=self.feature_names_, fill_value=0)\n",
    "\n",
    "        return X_full.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf900ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "preprocessor = PodcastPreprocessor()\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a7b448f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['podcast_model.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe.fit(train, y_train)\n",
    "preprocessor = PodcastPreprocessor()\n",
    "X_train_transformed = preprocessor.fit_transform(train)\n",
    "joblib.dump(preprocessor, \"podcast_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a53a20d",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3b8d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "xgb_base = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    random_state=SEED,\n",
    "    tree_method=\"hist\",   # or gpu_hist\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c5ead12",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    \"n_estimators\": [300, 500, 800, 1000],\n",
    "    \"max_depth\": [6, 8, 10, 12, 15],\n",
    "    \"learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.5, 0.7, 0.9],\n",
    "    \"reg_alpha\": [0.0, 0.5, 0.8, 1.0],\n",
    "    \"reg_lambda\": [1.0, 2.0, 4.0, 6.0],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40507e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dagshub\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri('https://dagshub.com/NitinNandeshwar/Podcast-Listening-Time-Prediction.mlflow')\n",
    "dagshub.init(repo_owner='NitinNandeshwar', repo_name='Podcast-Listening-Time-Prediction', mlflow=True)\n",
    "\n",
    "mlflow.set_experiment(\"xgboost_podcast_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523b852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/13 23:13:17 INFO mlflow.tracking.fluent: Experiment with name 'xgboost_podcast_regression' does not exist. Creating a new experiment.\n",
      "2025/12/13 23:13:18 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of xgboost. If you encounter errors during autologging, try upgrading / downgrading xgboost to a supported version, or try upgrading MLflow.\n",
      "2025/12/13 23:13:18 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n",
      "2025/12/13 23:13:18 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of lightgbm. If you encounter errors during autologging, try upgrading / downgrading lightgbm to a supported version, or try upgrading MLflow.\n",
      "2025/12/13 23:13:18 INFO mlflow.tracking.fluent: Autologging successfully enabled for lightgbm.\n",
      "2025/12/13 23:13:18 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2025/12/13 23:13:18 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run(run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgb_random_search\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     21\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mautolog()\n\u001b[1;32m---> 23\u001b[0m     \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_transformed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     best_rmse \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39msearch\u001b[38;5;241m.\u001b[39mbest_score_\n\u001b[0;32m     26\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:580\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    578\u001b[0m     patch_function\u001b[38;5;241m.\u001b[39mcall(call_original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 580\u001b[0m     patch_function(call_original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    582\u001b[0m session\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msucceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    584\u001b[0m try_log_autologging_event(\n\u001b[0;32m    585\u001b[0m     AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_patch_function_success,\n\u001b[0;32m    586\u001b[0m     session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    590\u001b[0m     kwargs,\n\u001b[0;32m    591\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:249\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001b[1;34m(original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     managed_run \u001b[38;5;241m=\u001b[39m create_managed_run()\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m     result \u001b[38;5;241m=\u001b[39m patch_function(original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;66;03m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;66;03m# that runs are terminated if a user prematurely interrupts training execution\u001b[39;00m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;66;03m# (e.g. via sigint / ctrl-c)\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m managed_run:\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\mlflow\\sklearn\\__init__.py:1653\u001b[0m, in \u001b[0;36m_autolog.<locals>.patched_fit\u001b[1;34m(fit_impl, allow_children_patch, original, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mshould_log():\n\u001b[0;32m   1650\u001b[0m     \u001b[38;5;66;03m# In `fit_mlflow` call, it will also call metric API for computing training metrics\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m     \u001b[38;5;66;03m# so we need temporarily disable the post_training_metrics patching.\u001b[39;00m\n\u001b[0;32m   1652\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _AUTOLOGGING_METRICS_MANAGER\u001b[38;5;241m.\u001b[39mdisable_log_post_training_metrics():\n\u001b[1;32m-> 1653\u001b[0m         result \u001b[38;5;241m=\u001b[39m fit_impl(original, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_log_post_training_metrics:\n\u001b[0;32m   1655\u001b[0m         _AUTOLOGGING_METRICS_MANAGER\u001b[38;5;241m.\u001b[39mregister_model(\n\u001b[0;32m   1656\u001b[0m             \u001b[38;5;28mself\u001b[39m, mlflow\u001b[38;5;241m.\u001b[39mactive_run()\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id\n\u001b[0;32m   1657\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\mlflow\\sklearn\\__init__.py:1428\u001b[0m, in \u001b[0;36m_autolog.<locals>.fit_mlflow\u001b[1;34m(original, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1426\u001b[0m _log_pretraining_metadata(autologging_client, \u001b[38;5;28mself\u001b[39m, X, y_true)\n\u001b[0;32m   1427\u001b[0m params_logging_future \u001b[38;5;241m=\u001b[39m autologging_client\u001b[38;5;241m.\u001b[39mflush(synchronous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1428\u001b[0m fit_output \u001b[38;5;241m=\u001b[39m original(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1429\u001b[0m _log_posttraining_metadata(autologging_client, \u001b[38;5;28mself\u001b[39m, X, y_true, sample_weight)\n\u001b[0;32m   1430\u001b[0m autologging_client\u001b[38;5;241m.\u001b[39mflush(synchronous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:561\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[1;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m         original_result \u001b[38;5;241m=\u001b[39m original(\u001b[38;5;241m*\u001b[39m_og_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_og_kwargs)\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n\u001b[1;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_original_fn_with_event_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_original_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:496\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[1;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     try_log_autologging_event(\n\u001b[0;32m    489\u001b[0m         AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_original_function_start,\n\u001b[0;32m    490\u001b[0m         session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m         og_kwargs,\n\u001b[0;32m    495\u001b[0m     )\n\u001b[1;32m--> 496\u001b[0m     original_fn_result \u001b[38;5;241m=\u001b[39m original_fn(\u001b[38;5;241m*\u001b[39mog_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mog_kwargs)\n\u001b[0;32m    498\u001b[0m     try_log_autologging_event(\n\u001b[0;32m    499\u001b[0m         AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_original_function_success,\n\u001b[0;32m    500\u001b[0m         session,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m         og_kwargs,\n\u001b[0;32m    505\u001b[0m     )\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:558\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[1;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[0;32m    555\u001b[0m     disable_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    556\u001b[0m     reroute_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    557\u001b[0m ):\n\u001b[1;32m--> 558\u001b[0m     original_result \u001b[38;5;241m=\u001b[39m original(\u001b[38;5;241m*\u001b[39m_og_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_og_kwargs)\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1959\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1959\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1961\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Suman\\anaconda3\\envs\\podcast\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=kf,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"xgb_random_search\"):\n",
    "    mlflow.autolog()\n",
    "\n",
    "    search.fit(X_train_transformed, y_train)\n",
    "\n",
    "    best_rmse = -search.best_score_\n",
    "    best_params = search.best_params_\n",
    "\n",
    "    mlflow.log_metric(\"best_cv_rmse\", best_rmse)\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    best_model = search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af682fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9e9142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true y min: 0.0\n",
      "true y max: 119.97\n",
      "true y mean: 45.43740628813335\n",
      "true y median: 43.37946\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "podcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
